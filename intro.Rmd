---
title: "Time-to-event analysis"
author: "Thomas Lumley"
date: "2017-9-12"
output: ioslides_presentation
---
```{r echo=FALSE}
library(survival)
```

## The fundamental problem

We are often interested in the time until some event happens

- death
- death from heart disease
- graduation
- getting a job
- HIV infection
- diabetes diagnosis

When we analyse the data, the event still hasn't happened for some people, or it has but we don't know about it.


## Introductions

Me: 

- epidemiology: cardiovascular disease, genomics
- computing: ported the S-PLUS 'survival' package to R, wrote a LISP-Stat survival package
- theory: survival analysis in subsamples, eg, case-cohort designs.

You:

- what area of application?
- what are you hoping to get out of the course?



## Course content


- Concepts: censoring, truncation, competing risks, choice of time scale
- Summaries: the Kaplan--Meier curve; mean, median, and proportion surviving; the hazard rate; graphical exploration
- Two-sample testing: the logrank test and its strengths and weaknesses
- The proportional  hazards model: right censoring, left truncation,
- Time-varying predictors
- Modelling recurrent events
- (briefly) parametric accelerated-failure models
- (briefly) Interval censoring
- (briefly) Case-cohort and countermatched designs

## Course structure

Four 1.5 hour blocks each day

Mostly: 

- 45 min lecture
- 30 min practical
- 15 min discussion

Ask questions, or I won't know what I'm not explaining right.

## Maths warning

There will be equations, because they help some people understand some things.

You can ignore them if you don't like them.

They won't be on the exam.

## Software

I speak R and some Stata

Yannan speaks SAS

Everything we talk about can be done in R, most of it can be done in SAS or Stata. 

We can try to help you with SPSS or other software. 

## Questions?

<img src="weka_hires.jpg" height=500">

## The basic case: right censoring

How long do people survive after surgery for colon cancer with three treatment regimens?

- time starts at surgery
- time ends when someone dies
- they might still be alive when we do the analysis

## Notation: 

- $X$: 'True' survival time 
- $C$: follow-up time -- ie, time until data lock for analysis
- $T$: observed time = $\min(X, C)$
- $\Delta$: Event indicator: $\Delta=1$ if you died at $T$, $\Delta=0$ if you were alive at $T$


We say the time is **right censored** at $T$ for people still alive

- *censored*: the time of death still exists, we just don't get to see it
- *right*: time flowing from left to right; it's the right-hand end we don't see

## Analyses

- **Just look at $\Delta$**: loses information. Biased if $C$ is different by treatment
- **Just look at $X$**: loses information. Biased if $C$ is different by treatment

We need to look at both. Two questions:

- what do we want to do?
- how do we tell the computer to do it?

## Discussion

How are...

- death from heart disease
- graduation
- getting a job
- HIV infection
- diabetes diagnosis

...more complicated when defining time-to-event?

**Pick a couple and talk to the person next to you for 2 minutes**

## Back to the basic case

Want to estimate survival function $S(t)= P(X\geq t)$

But we **don't know** if $X_i\geq t$ when $t$ is after the censoring time.

Three approaches...

- small intervals
- imputation
- maximum likelihood

... all give the same result.

## Data example

Survival from diagnosis of primary biliary cirrhosis

- rare liver disease
- not (at the time) treatable
- data from before transplants were routine
- follow-up ranging from two to 13 years.

## Discrete time

Round the times to each 5 years

- compute proportion dying for each point: deaths/number under observation
- compute $S(t)$ by multiplying $1-P(\textrm{dying})$ for each time point up to $t$

Biased: the denominator actually changes over the 5 years

What happens as the time intervals get shorter: 10, 5, 2, 1, 1/2, 1/12...

## Example: individual times

```{r echo=FALSE}
library(survival)
data(pbc)

dies<-function(time, status, from, to){
	keep<-time>from
	time<-time[keep]
	status<-status[keep]
	status[time>to]<-0 #I aten't dead
	mean(status)
}

t10<-with(pbc, dies(time/365, status, from=0, to=10))


t5<-with(pbc,c(dies(time/365,status,0,5),
	dies(time/365,status,5,10)))



t2<-with(pbc, c(dies(time/365,status,0,2),
	dies(time/365,status,2,4),
	dies(time/365,status,4,6),
	dies(time/365,status,6,8),
	dies(time/365,status,8,10)))
	
t1<-sapply(0:9, function(from) with(pbc, dies(time/365,status, from,from+1)))	

thalf<-sapply(seq(0,9.5,by=0.5), function(from) with(pbc, dies(time/365,status, from,from+0.5)))	

tmonth<-sapply(seq(0,9+11/12,by=1/12), function(from) with(pbc, dies(time/365,status, from,from+1/12)))	

## Individual points get noisier
plot(1:10,t1,type="h",xlab='Years',ylab="Proportion dying",ylim=range(0,t1))
```

---

```{r echo=FALSE}
plot(seq(1/12,10,by=1/12),tmonth,type="h",xlab='Years',ylab="Proportion dying")
```

## Example: survival function

```{r echo=FALSE}
## survival function doesn't
plot(c(0,10), c(1,cumprod(1-t10)),type="s",xlab="years",ylab="Proportion alive", ylim=c(0,1))
lines(c(0,5, 10), c(1, cumprod(1-t5)),type="s",lty=2)
lines(c(0,2,4,6,8,10), c(1, cumprod(1-t2)),type="s",lty=3)

lines(c(0:10), c(1, cumprod(1-t1)),type="s",lty=1,col="blue")
lines(seq(0,10,by=0.5), c(1, cumprod(1-thalf)),type="s",lty=3,col="blue")
lines(seq(0,10,by=1/12), c(1, cumprod(1-tmonth)),type="s",lty=1,col="goldenrod")
```


## Shorter intervals

- Estimate of $P(\textrm{dying})$ in an interval gets noisy for short intervals
- Bias decreases
- Cumulative survival **doesn't** get noisier (proportions kinda sorta independent)
- Can take intervals all the way down to single events!

Kaplan-Meier estimator

$$\hat{S}(t) = \prod_{s\leq t} \left(1-\frac{\textrm{deaths at }s}{\textrm{alive before }s}\right)$$

## Imputation

- Start off with $1/n$ probability at each time point
- Moving through time, when you get to a censored time, spread its probability evenly over all future time points

Ends up with Kaplan--Meier estimator

## Example

(in R)


## Maximum likelihood

What $\hat S()$ makes the likelihood of the data set highest?

- deaths only at observed death times
- If $\Delta=1$, contribution is size of step (predicted proportion dying) at $t_i$ 
- If $\Delta=0$, contribution is total of *future* step sizes

Choosing the step sizes to maximise the likelihood also gives the Kaplan-Meier estimator

## Summaries

- The whole curve $\hat{S}(t)$
- The median (where $\hat{S}(t)=0.5$) and other quantiles
- Survival at particular times (eg, five years, one year, 30 days)
- The **restricted** mean: area under the $\hat{S}(t)$ curve up to some time limit (eg five years)

## Exercise

You, too, can compute the Kaplan--Meier estimator


## Questions?

<img src="weka_hires.jpg" height=500">


## Censoring assumptions

Basic assumption: the probability of dying at time $t$ is the same for someone censored before $t$ as for someone being observed at $t$.

In regression models, can weaken this to: the probability of dying at time $t$ is the same for someone censored before $t$ as for someone being observed at $t$ who has the same covariate values.

Formal name: **Non-informative censoring** or **Ignorable Censoring** or **Censoring At Random**

---

It is **impossible** to get an unbiased estimate of ${S}(t)$ without the ignorable-censoring assumption.

You **cannot** test this assumption using the data. 

Sometimes, you can estimate something else useful instead of $S(t)$ (competing risks; tomorrow)


## Common two-sample comparisons

- Median survival
- **% surviving five years** (say)
- restricted mean survival
- **hazard rates**

## Survival at a point

- Estimate $S(t)$ in each group: $\hat{S}_A(t)$, $\hat{S}_B(t)$
- Difference: $\hat{S}_A(\textrm{5 years})-\hat{S}_B(\textrm{5 years})$ has Normal distribution
- Variance of difference: $$\mathrm{var}[\hat{S}_A(\textrm{5 years})]+\mathrm{var}[\hat{S}_A(\textrm{5 years})]$$

Need variances. Easiest to go via the **hazard function**


## Hazard

Rate at which events are happening (eg %/year). Written $h(t)$ or $\lambda(t)$

- Average rate over time:  $(\sum_i \Delta_i)/(\sum_i T_i)$, eg 161 deaths in 2196 person-years of followup gives 7%/year death rate
- As a function of time: is it going up or down? What is the rate at 1 year?

Estimation involves smoothing over short windows of time

## Example:PBC

```{r echo=FALSE}
library(muhaz)
with(pbc, plot(muhaz(time/365, status==2)))
```

## Cumulative Hazard

Written capital $H(t)$ or $\Lambda(t)$

- adding up the hazard over time
- expected number of deaths if people didn't go away when they died
- also $H(t)=-\log S(t)$

Estimator: (Nelson-Aalen)

$$\hat\Lambda(t) = \sum_{s\leq t} \frac{\textrm{deaths at }s}{\textrm{alive before }s}$$


## Example

```{r}
plot(survfit(Surv(time/365,status==2)~1,data=pbc),fun="cumhaz",
     xlab="Years",ylab="Cumulative Hazard")
```

## Related to survival

$$\Lambda(t)= -\log S(t)$$
and
$$\hat{\Lambda}(t)\approx -\log\hat{S}(t)$$
because
$$\log\prod_{s\leq t}\left(1-\frac{\textrm{died}}{\textrm{at risk}}\right)=\sum_{s\leq t}\log\left(1-\frac{\textrm{died}}{\textrm{at risk}}\right)\approx -\sum_{s\leq t} \frac{\textrm{died}}{\textrm{at risk}}$$

## Variance of hazard

Treat each time point as independent Binomial 

$$\widehat{\mathrm{var}}[\hat{\Lambda}(t)]=\sum_{s\leq t} \frac{\textrm{deaths at }s}{\textrm{alive before }s}\left(1-\frac{\textrm{deaths at }s}{\textrm{alive before }s}\right)\frac{1}{\textrm{alive before }s}$$

Not **really** independent, but it **is** valid

So, we can get $\widehat{\mathrm{var}}[\hat{S}(t)]$ from the $S(t)=\exp( -\Lambda(t))$ relationship, and compare $\hat{S}(t)$ between groups.

Or, work on the log scale and compare $\hat{\Lambda}(t)$ between groups: better small-sample behaviour




## More notation

- $N_i(t)$ number of times person $i$ has died by time $t$
- $dN_i(t)$ number of times person $i$ dies at exactly time $t$
- $\bar N(t)$ total number of deaths observed by time $t$
- $Y_i(t)$ is person $i$ under observation at time $t$?
- $\bar Y(t)$ total number of people under observation at time $t$

For right-censored survival data

- $N_i(t)=0$ before $T_i$. After $T_i$, $N_i(t)=\Delta_i$
- $Y_i(t)=1$ until $T_i$, then $Y_i(t)=0$

Looks like pointless complication

## Two reasons

Allows for more complicated processes, eg:

- asthma attacks: $N_i=0,1,2,3,\dots,\textrm{many}$
- getting a job: $Y_i(t)=1$ only while you're looking for a job

Allows some advanced probability theory to be used in proofs

- 'kinda sorta independent' means 'martingale'
- martingales behave a lot like sums of independent things.


## Notation for sums

$$\hat\Lambda(t) = \int_0^t \frac{d\bar N(s)}{\bar Y(s)}$$

It just means
$$\hat\Lambda(t) = \sum_{s\leq t} \frac{\textrm{deaths at }s}{\textrm{alive before }s}$$


## The logrank test

- Compute $d\bar N(s)$ in two groups and take the difference
- Add up over time, weighted according to how much information is available

$$Q = \int_0^\infty w(t)\left(\frac{d\bar N_A(s)}{\bar Y_A(s)}-\frac{d\bar N_B(s)}{\bar Y_B(s)}\right)$$

- If the difference is in a consistent direction, $Q$ will be big
- $Q$ will have a Normal distribution, similar trick for estimating its variance.
- People usually quote $Q^2/\mathrm{var}[Q]$, not $Q$, which has a $\chi^2_1$ distribution

## Weights

If the relative difference is roughly constant, (proportional hazards), we care about how much information is available at each time:

- Want $$\frac{1}{w(s)}\propto\frac{1}{\bar Y_A(s)}+\frac{1}{\bar Y_B(s)}$$

If the relative difference increases or decreases over time, multiply by some increasing or decreasing function of time:

- $\hat{S}(s)^\rho(1-\hat{S}(s))^\gamma$ is a convenient family of functions that can be increasing, decreasing or both

## Exercise

Comparing two groups

## Questions?

<img src="weka_hires.jpg" height=500">

---

Lunch: 

## Regression models

- The Cox model
- accelerated failure models

## The proportional hazards model

$S(t)$ has upper and lower limits, but $\log \Lambda(t)$ or $\log\lambda(t)$ don't

A simple regression model for rate of death at a given time
$$\log \lambda(t;z) = \log\lambda_0(t)+z\beta$$

$\beta$ are log hazard ratios: $e^\beta$ is the ratio of hazards for two people with 1-unit difference in $z$.  The model says this ratio is constant over time. 

$\lambda_0(t)$ is the hazard at time $t$ for someone with $z=0$.

## The Cox model

We could choose a form for $\lambda_0(t)$, eg constant or a power of $t$.

David Cox observed that we don't have to: we can estimate $\beta$ **without** estimating $\lambda_0$

Suppose someone dies at time. The probability it's person $i$ is 
$$p(\beta)= \frac{\lambda(t;z_i)}{\sum_{\textrm{alive at } t}\lambda(t;z_j)}=\frac{\lambda_0(t)e^{z_i\beta}}{\sum_{\textrm{alive at } t} \lambda_0(t)e^{z_j\beta}}=\frac{e^{z_i\beta}}{\sum_{\textrm{alive at } t} e^{z_j\beta}}$$

Multiplying these over all times where a death occurs gives something like a likelihood: the Cox partial likelihood.

## Digression: partial likelihood

Subsequent research shows

- The partial likelihood behaves exactly like a likelihood: estimation, testing, etc
- You can't get a more precise estimator without making assumptions about $\lambda_0(t)$
- Even making assumptions doesn't gain you much

There are basically **no other** examples of a fully efficient estimator that ignores an infinite-dimensional parameter. 

## Implications

Partial likelihood compares individuals **at the same time**

- no information from between-time comparisons
- no confounding by time: perfect adjustment
- the value of the time isn't used, only the ordering: transformations of time don't matter
- (there's no link between $z_i$ at different values of $t$)

## Example

Randomised trial of chemotherapy for colon cancer

Three treatment groups

- control
- levamisole
- levamisole + 5-fluorouracil

Also measured age, sex, things about the tumour.

We have time to recurrence and time to death.

## Simple comparisons


```{r}
data(colon)
deaths<-subset(colon, etype==2)
recur<-subset(colon, etype==1)

coxph(Surv(time,status)~I(rx=="Obs"), data=deaths)
```

---

```{r}
coxph(Surv(time,status)~rx, data=deaths)
```

## Cox vs logrank
 
Cox model tests for binary variable very similar to logrank test

- logrank test is **exactly** the score test for the partial likelihood
- *cf* Mantel-Haenszel test and conditional logistic regression

In two-group case, estimate $\hat\beta$ solves
$$0=\int_0^\infty w(t)\left(\frac{d\bar N_A(s)}{\bar Y_A(s)}-e^\beta\frac{d\bar N_B(s)}{\bar Y_B(s)}\right)$$
for same $w(t)$ as logrank test

## Age and interaction

```{r}
coxph(Surv(time,status)~rx*age, data=deaths)
```

What do the coefficients mean now?

## Age and interaction

```{r}
coxph(Surv(time,status)~rx*I(age-60), data=deaths)
```

What do the coefficients mean now?

## Estimating survival

We don't need $\lambda_0(t)$ to estimate $\beta$, but we do to predict $S(t;z)$.

(Aalen-)Breslow estimator similar to Nelson-Aalen estimator, but giving each person a weight of $e^{z\beta}$

$$\hat\Lambda_0(t) = \int_0^t \frac{ d \bar N(s)}{\sum_i e^{z_i\beta} Y_i(s)}$$

Compute $\hat{\Lambda}(t;z_i)$ for an individual $i$ by multiplying by $e^{z_i\beta}$

Compute $\hat{S}(t;z_i)$ as $\exp(-\Lambda(t))$


## Example: survival curves

```{r}
mtrt<-coxph(Surv(time/365,status)~rx,data=subset(colon, etype==2))
plot(survfit(mtrt, newdata=data.frame(rx=c("Obs","Lev","Lev+5FU"))))
```

## Compared to Kaplan-Meier

```{r}
plot(survfit(mtrt, newdata=data.frame(rx=c("Obs","Lev","Lev+5FU"))))
lines(survfit(Surv(time/365,status)~rx,data=subset(colon, etype==2)),
      col="blue")
```

## Scatterplots 

```{r}
plot(time~age,pch=ifelse(status==1,20,1),
     col=ifelse(status==1, "blue","gray"),data=deaths)
```

## Quantile smoothing

Estimate $S(t;\textrm{age}=60)$ with a Kaplan-Meier estimator using just points near $\mathrm{age}=60$.

Even better, weight points according to the age difference from $\mathrm{age}=60$.

Repeat for different values of 60, extract some quantiles



## Kaplan-Meier contour plot


```
source("kmq.R")
plot(time~age,pch=ifelse(status==1,20,1),
     col=ifelse(status==1, "blue","gray"),data=deaths)
qq<-with(deaths, kmquantiles(time,status, age,p=c(0.9,0.75,0.5)))
lines(qq$x,qq$y["0.9",])
lines(qq$x,qq$y["0.75",])
lines(qq$x,qq$y["0.5",])
```

---

```{r echo=FALSE}
source("kmq.R")
plot(time~age,pch=ifelse(status==1,20,1),
     col=ifelse(status==1, "blue","gray"),data=deaths)
qq<-with(deaths, kmquantiles(time,status, age,p=c(0.9,0.75,0.5)))
lines(qq$x,qq$y["0.9",])
lines(qq$x,qq$y["0.75",])
lines(qq$x,qq$y["0.5",])
```


## Exercise

Your turn, with a famous survival data set

## Questions?

<img src="weka_hires.jpg" height=500">

## Time scales

Cox model compares people with different covariates *at the same point in time*: so far, time since start of study

Could equally be:

- time since birth
- calendar time
- time since exposure started
- time since last event

Which one do you want perfect adjustment for?

## Left truncation

For some time scales (eg age), we don't have everyone under observation at the start of time

- people enter the study at some starting age
- the study ends at some stopping age 
- they might die at some age in between

We say there is *left truncation* at the entry time

- *left*: time passing from left to right
- *truncation*: we don't get to see people who die before the starting age. They aren't in the data set.

## Estimation

- Can't estimate $S(t)$ at ages before we have data
- Can estimate decreases in $S(t)$ after we have data
- Can estimate hazards after we have data
- Can estimate hazard ratios after we have data

Define $Y_i(s)=1$ between the starting time and death or ending time, $Y_i(s)=0$ otherwise. All the same formulas work. 

We get left truncation for free!

## But wait!

Nothing in the formulas requires $z_i$ to be the same at different times. We can rewrite the partial likelihood as

$$p(\beta)= \frac{\lambda_0(t)e^{z_i(t)\beta}}{\sum_{j\textrm{ alive at } t} \lambda_0(t)e^{z_j(t)\beta}}=\frac{e^{z_i(t)\beta}}{\sum_{j\textrm{ alive at } t} e^{z_j(t)\beta}}$$

Now we are asking about how the **current** value of $z$ affects the hazard of death, rather than the value of $z$ at time 0.

We also get time-varying covariates for free<sup>*</sup>!

<sup>*</sup> <sup>terms and conditions apply</sup>


## Computation

The simplest way to  allow covariates to change is multiple records per person (R, Stata, SAS)

Each record has 

- start time
- stop time
- event status at the stop time
- current covariate values

## Examples

Stanford Heart Transplant program: survival for people entered on the waiting list

Two periods:

- start at waiting list entry, end at transplant or death, `transplanted=0`
- start at transplant, end at death or end of follow-up,`transplanted=1`
- plus time-constant baseline covariates

---

Impact of protection orders on intimate partner violence: (Holt et al, JAMA 2002)

- initial event to grant of protection order (`order=none`)
- first six months of protection order (`order=recent`)
- after first six months (`order=established`)
- after end of order (`order=none`)
- plus time-constant baseline covariates

---

Adherence to medications as a predictor of stroke

- `adherent=TRUE` starting when a new prescription is filled.
- `adherent=FALSE` when it should have run out (at 80% adherence)
- other covariates (eg other illness) updated according to medical records

## But what does it mean

- No use for prediction (except if we have future covariates)
- Endogeneity (consider `heart_is_beating=TRUE/FALSE`)

## Exercise

Time scales and time-varying predictors

## Questions?

<img src="weka_hires.jpg" height=500">

## There's more!

Nothing in the formulas requires people to go away after they have an event. We don't even need to rewrite the partial likelihood

$$p(\beta)= \frac{\lambda_0(t)e^{z_i(t)\beta}}{\sum_{j\textrm{ observed at } t} \lambda_0(t)e^{z_j(t)\beta}}=\frac{e^{z_i(t)\beta}}{\sum_{j\textrm{ observed at } t} e^{z_j(t)\beta}}$$

Now we are asking about how the **current** value of $z$ affects the rate of events.

Define $Y_i(s)=1$ between the starting time and ending time, $Y_i(s)=0$ otherwise. All the same formulas work. 


We also recurrent events for free<sup>*</sup>!

<sup>*</sup> <sup>terms and conditions apply</sup>

