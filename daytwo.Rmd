---
title: "Time-to-event analysis"
author: "Thomas Lumley"
date: "2017-9-13"
output: ioslides_presentation
---

## Questions?

<img src="weka_hires.jpg" height=500">

## Recurrent events

Nothing in the formulas requires people to go away after they have an event. We don't even need to rewrite the partial likelihood

$$p(\beta)= \frac{\lambda_0(t)e^{z_i(t)\beta}}{\sum_{j\textrm{ observed at } t} \lambda_0(t)e^{z_j(t)\beta}}=\frac{e^{z_i(t)\beta}}{\sum_{j\textrm{ observed at } t} e^{z_j(t)\beta}}$$

Now we are asking about how the **current** value of $z$ affects the rate of events.

Define $Y_i(s)=1$ between the starting time and ending time, $Y_i(s)=0$ otherwise. All the same formulas work. 


We also get recurrent events for free<sup>*</sup>!

<sup>*</sup> <sup>terms and conditions apply</sup>

## Computation

Multiple records, just as for time-varying predictors

More than one record for a given person can end with an event

We need an `id` variable to track which records are the same person

## Issues

**Correlation**: people who have had an event may be at higher risk of a second

- because the first event really increased their risk (eg sprain)
- because the first event revealed they were at high risk (eg asthma)

Easy to handle: variance estimator using `id` variable, similar to GEE for longitudinal data

- `+cluster(id)` in model formula for R
- `, cluster(id)` option for Stata
- `ID` statement and `COVS(AGGREGATE)` option in SAS

---

**Time scale**: should we be comparing

- people at the same time after a fixed time zero? (eg, same age). 
- people at the same time after their previous event? (eg, same recovery time). 

Either can make sense: need to think about which time variable it's most important to adjust for. 

## Data setup

Someone has events at times 5, 7, 12, and is censored at time 15

- start 0, stop 5, event=1
- start 5, stop 7, event=1
- start 7, stop 12, event=1
- start 12, stop 15, event=0

or 

- start 0, stop 5, event=1
- start 0, stop 2, event=1
- start 0, stop 5, event=1
- start 0, stop 3, event=0



## Clustered data

- two eyes of same person
- ipsilateral vs contralateral recurrence of deep vein thrombosis
- twins/triplets/etc
- household members

## Issues

Do you want **matched** comparisons only within groups, or is clustering just a nuisance?

- we care about the difference between same-leg and opposite-leg recurrence of deep vein thrombosis
- we're studying babies, and a few percent of them happen to be multiples. 


## Matched (stratified) Cox model

If we care about comparisons only within a pair, we should use only data from within a pair in the partial likelihood

Label the people in the $i$th pair as $i0$ and $i1$. If $i0$ has an event before $i1$
$$p(\beta)= \frac{\lambda_0(t)e^{z_{i0}(t)\beta}}{ \lambda_0(t)e^{z_{i0}(t)\beta}+ \lambda_0(t)e^{z_{i1}(t)\beta}}=\frac{e^{z_{i0}(t)\beta}}{e^{z_{i0}(t)\beta}+e^{z_{i1}(t)\beta}}$$

More generally, to do comparisons within a **stratum**, the denominator is only a sum over people in that stratum under observation at the same time.

Implies a separate (not estimated) baseline $\lambda_0(t)$ for each stratum


---

Computation:

- `+strata(id)` in R
- `, strata(id)` in Stata
- `STRATA id;` statement in SAS PHREG

Can also be used to handle non-proportional hazards for a discrete variable: comparisons only within the same level of that variable.

## Clustered Cox model

If the clustering is a nuisance, not the primary question, we just want a Cox model for individual observations as usual

Use

- `+cluster(id)` in model formula for R
- `, cluster(id)` option for Stata
- `ID` statement and `COVS(AGGREGATE)` option in SAS

to get correct standard errors.

## More complicated

We might care about estimating the **correlation** within a cluster

- are monozygotic (identical) twins more similar than dizygotic (fraternal) twins?
- are you more likely to graduate late if your flatmates do?

This is harder, and the models depend on context.  The R `coxme` package handles heritability estimation for family-based genetics. 

## Exercise

<img src="exercise.png" height=400px>


## Questions?

<img src="weka_hires.jpg" height=500">


## Competing risks

Example: tumours are removed surgically, and the surgeon is interested whether the removal is complete enough

- surgery couldn't do anything about metastases
- surgeon wants the time to *local* recurrence, censored at death or distant recurrence

Example: blood pressure treatment reduces risk of stroke and heart disease

- researchers want time to stroke or heart attack
- deaths from other causes shouldn't be affected by treatment, use as censoring time

## Problem

Ignorable censoring assumption would say that someone who died of another cause was still at the same risk of stroke or heart attack or local recurrence as someone still alive

They aren't. They're dead. 

Ignorable censoring **can't be** true in the usual sense: competing risks aren't really censoring.

**Conceivable** that someone who died of another cause would still be at the same risk of stroke or heart attack or local recurrence as someone still alive, if they hadn't died.  Unlikely.

## Ignorable?

Why might...

- someone who had a distance cancer recurrence have been at a higher or lower risk for local recurrence?
- someone who had a heart attack have been at a higher or lower risk for cancer?

...talk to the person next to you for a couple of minutes.

## What can we estimate?

**Crude incidence**: what proportion of people get/don't get local recurrence or stroke by time $t$? Crude incidences add up to total incidence: $1-\hat{S}(t)$

**Cause-specific hazard**: rate of events from a specific cause. Cause-specific hazards add up to total hazard : $\hat{\Lambda}(t)$

Both involve removing the effect of actual censoring, but not of competing risks. 

## Not the same

Example: 

- group 1: rates of  30%/year for cancer and non-cancer deaths
- group 2: cancer rate of 20%/year, non-cancer rate of 10%/year


Cause-specific hazard of cancer is higher in group 1, but more people eventually die of cancer in group 2 because they don't die of other things first. 

Crude incidence and cause-specific hazard need not agree unless all event probabilities are low.

---

```{r echo=FALSE}
par(mfrow=c(2,1),mar=c(4,4,.1,1))
curve(3+0*x,from=0,to=3,ylim=c(0,4),xlab="Decades",ylab="Rate")
curve(1+0*x,col="red",add=TRUE)
curve((1-exp(-6*x))/2,from=0,to=3,xlab="Decades",ylab="Cumulative Incidence",ylim=c(0,1))
curve((1-exp(-3*x))*2/3,col="red",add=TRUE)

```

## Computation

- Cause-specific hazards are what you estimate if you do a Cox model 'censoring' at deaths from other causes
- Crude incidence is what you estimate if you say someone dying of one cause never dies of other causes and is censored at the end of the study -- but that gets the standard errors wrong.

R package `cmprsk` does crude-incidence comparisons and regression models

`-stcrreg-` and `-stcurve-` in Stata'

`eventcode` option to `PHREG` in SAS

## Questions?

<img src="weka_hires.jpg" height=500">


## Ties,  intervals, error

We've assumed that

- we know the start time for each person
- we know the censoring time for each person (if they don't die)
- we know the event time for each person (if they are under observation)

These aren't always true.  

We've also *implicitly* assumed all the event times (in a stratum) are different when writing down the partial likelihood.

## Ties from rounding

Times may get rounded off to the nearest day, week, month.

Software uses various approximations to the partial likelihood. They are all Good Enough except in extreme cases.

R and Stata default to the same approximation; SAS defaults to a cruder one. 

Worth knowing about as a potential explanation if you get slightly different results from the same data with different programs.

## Events only at visit

Some events can only be measured retrospectively by a diagnostic test

- incident diabetes
- HIV infection

We don't know the time ordering of the real events between diagnostic tests, so we don't know who goes in the denominator for the partial likelihood at each event.

If everyone has similar test schedule or test schedule is frequent enough, just pretend the events happen at the test times. 

Otherwise: interval censoring is hard, especially if the covariate of interest is related to test schedule.

## Retrospective assessment

Example: heart attack adjudicated by expert examination of medical records

- date and time is known immediately
- whether it is an event isn't known until later.

Example: vital status from  death certificate records

- nothing known immediately
- date and time will be known, but not for a while

The censoring time should be the last time you **know** the event hadn't happened

## Measurement error

- event may be false positive
- event may be missed
- start time may be wrong (recall error, eg start time for antiretroviral treatment in HIV)

These are hard if the errors are large/frequent.  Ask an expert. 


## Exercise

<img src="exercise.png" height=400px>


## Questions?

<img src="weka_hires.jpg" height=500">


## Beyond proportional hazards

Why is the Cox model so popular?

- Computationally well-behaved (important in 1980s)
- Close relationship to logistic regression in case-control studies
- Lots of easy extensions
- Mathematically interesting
- Any model is ok when effects are weak

Not because it necessarily fits well. 

## Diagnostics

Useful

- compare predicted survival from the model to the Kaplan-Meier estimator
- smoothed Schoenfeld residual plots

Useful but computationally annoying

- explicit interactions with time

Useless but widely available

- loglog plots (plots of $-\log\Lambda(t)$ by discrete variables)


## Schoenfeld residuals

Given $z_j$ for people at risk at time $t$, we can compute an *average* expected $z_i$ for the person who has an event
$$e(t) = \frac{\sum_{i\textrm{ at risk}}z_j(t)e^{z_j(t)\beta}}{\sum_{j\textrm{ at risk}} e^{z_j(t)\beta}}$$
On average, the person with event will have 'worse' covariates than the whole cohort

We can compare $e(t)$ to the covariates for the person who actually had an event, say, $z_i(t)$.

The *Schoenfeld residuals* are $r(t)=z_i(t)-e(t)$. They have zero mean if the model is correct, and tend to have time trends if proportional hazards is not true.

## Using them

Formal test for correlation between $r(t)$ and some function of $t$.

- R: `cox.zph()`
- Stata: `estat phtest` after fitting a Cox model
- SAS: need to do some work.

Plot a scaled version of the residuals against time, use a smoother to estimate $\beta(t)$

## Example

```{r}
library(survival)
data(pbc)

model<-coxph(Surv(time,status==2)~log(bili)+protime+albumin, data=pbc)
test<-cox.zph(model)
test
```

---


```{r}
par(mfrow=c(1,2),mar=c(4,4,.1,1))
plot(test[1])
plot(test[2])
```

## Useless approach

- Divide `protime` into categories
- Fit a stratified Cox model
- Estimate the baseline cumulative hazard $\Lambda_0(t)$ for each stratum
- Plot the estimates $-\log\Lambda(t)$
- If proportional hazards is true, the curves should be parallel. 

Useless because you can't judge parallelness of curves (even without sampling noise)

## Example

```
m2<-coxph(Surv(time,status==2)~log(bili)+albumin+strata(protime>10),
          data=pbc)
plot(survfit(m2,
      newdata=data.frame(bili=c(3,3),protime=c(9,12),albumin=c(3.5,3.5))),
     fun="cloglog")
plot(survfit(m2,
      newdata=data.frame(bili=c(3,3),protime=c(9,12),albumin=c(3.5,3.5))),
     fun="cloglog", conf.int=TRUE,col=c("blue","goldenrod"))
```

---

```{r echo=FALSE}
m2<-coxph(Surv(time,status==2)~log(bili)+albumin+strata(protime>10),data=pbc)
plot(survfit(m2, newdata=data.frame(bili=c(3,3),protime=c(9,12),albumin=c(3.5,3.5))),
     fun="cloglog")
```

---

```{r echo=FALSE}
plot(survfit(m2,newdata=data.frame(bili=c(3,3),protime=c(9,12),albumin=c(3.5,3.5))),
     fun="cloglog", conf.int=TRUE,col=c("blue","goldenrod"))
```

## Accelerated-failure models.

Instead of modelling risk, model time: some people 'age faster' than others

$$\log T = \log T_0+z\beta$$
$e^\beta$ is the proportional difference in survival time for a one-unit difference in $z$.

We can't (easily) avoid modelling $T_0$: we need to chose a parametric distribution. 

## Simple cases

If the rate is constant over time then both proportional haazards and accelerated failure are true

- $\lambda_0(t)=\lambda$
- $P(T_0>t)=e^{-\lambda t}$

If the rate has a step increase

- under proportional hazards, different $z$ give different step increases at the same time
- under accelerated failure, different $z$ give step increases at different times. 

## Why?

- want to extrapolate to where there isn't much data (ok with a good model)
- tradition: eg in engineering reliability analysis
- want to get some Normal distributions in there for more complicated modelling ($\log T_0\sim N(\mu,\sigma^2)$)
- left censoring or interval censoring are easier with parametric models

(Note: left **censoring** means a person is in the data set but had the event before they entered)

## Example

For right-censored data, not very different from Cox model

```{r}
coxph(Surv(time,status==2)~log(bili)+protime+albumin, data=pbc)
```

---

```{r}
summary(survreg(Surv(time,status==2)~log(bili)+protime+albumin,
                data=pbc,dist="weibull"))
```

---


```{r}
summary(survreg(Surv(time,status==2)~log(bili)+protime+albumin,
                data=pbc,dist="lognormal"))
```

## Interval censoring

Data on 144 mice raised in a germ-free environment (48 mice) or a conventional environment (96 mice)

At death, they were examined to see if they had lung tumours.  

If a tumor was present, we know it developed before that time If not, the time until it would have developed is after that time (assumptions)

Define a starting time of `NA` (if present) or the death time (if absent) and an ending time of the death time (if present) or `NA` if absent

---

```{r}
load("lungtumor.rda")
summary(survreg(Surv(ltime,rtime,type="interval2")~group,data=lungtumor,
                dist="exponential"))
```

---

```{r}
exp(-1.07)
```

Mice in the germ-free environment developed cancer about three times as fast (or at 1/3rd the age) of those in the conventional environment.

---

Computation

- Stata `-intreg-` fits this model with a Normal distribution. The add-on module `-intcens-` adds other distributions: `ssc install intcens` to download it.
- SAS `PROC LIFEREG` fits this model

## Exercise

<img src="exercise.png" height=400px>


## Questions?

<img src="weka_hires.jpg" height=500">

## Subsampling


