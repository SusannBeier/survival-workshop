---
title: "Time-to-event analysis"
author: "Thomas Lumley"
date: "2017-9-13"
output: ioslides_presentation
---

## Questions?

<img src="weka_hires.jpg" height=500">

## Recurrent events

Nothing in the formulas requires people to go away after they have an event. We don't even need to rewrite the partial likelihood

$$p(\beta)= \frac{\lambda_0(t)e^{z_i(t)\beta}}{\sum_{j\textrm{ observed at } t} \lambda_0(t)e^{z_j(t)\beta}}=\frac{e^{z_i(t)\beta}}{\sum_{j\textrm{ observed at } t} e^{z_j(t)\beta}}$$

Now we are asking about how the **current** value of $z$ affects the rate of events.

Define $Y_i(s)=1$ between the starting time and ending time, $Y_i(s)=0$ otherwise. All the same formulas work. 


We also get recurrent events for free<sup>*</sup>!

<sup>*</sup> <sup>terms and conditions apply</sup>

## Computation

Multiple records, just as for time-varying predictors

More than one record for a given person can end with an event

We need an `id` variable to track which records are the same person

## Issues

**Correlation**: people who have had an event may be at higher risk of a second

- because the first event really increased their risk (eg sprain)
- because the first event revealed they were at high risk (eg asthma)

Easy to handle: variance estimator using `id` variable, similar to GEE for longitudinal data

- `+cluster(id)` in model formula for R
- `, cluster(id)` option for Stata
- `ID` statement and `COVS(AGGREGATE)` option in SAS

---

**Time scale**: should we be comparing

- people at the same time after a fixed time zero? (eg, same age). 
- people at the same time after their previous event? (eg, same recovery time). 

Either can make sense: need to think about which time variable it's most important to adjust for. 

## Data setup

Someone has events at times 5, 7, 12, and is censored at time 15

- start 0, stop 5, event=1
- start 5, stop 7, event=1
- start 7, stop 12, event=1
- start 12, stop 15, event=0

or 

- start 0, stop 5, event=1
- start 0, stop 2, event=1
- start 0, stop 5, event=1
- start 0, stop 3, event=0



## Clustered data

- two eyes of same person
- ipsilateral vs contralateral recurrence of deep vein thrombosis
- twins/triplets/etc
- household members

## Issues

Do you want **matched** comparisons only within groups, or is clustering just a nuisance?

- we care about the difference between same-leg and opposite-leg recurrence of deep vein thrombosis
- we're studying babies, and a few percent of them happen to be multiples. 


## Matched (stratified) Cox model

If we care about comparisons only within a pair, we should use only data from within a pair in the partial likelihood

Label the people in the $i$th pair as $i0$ and $i1$. If $i0$ has an event before $i1$
$$p(\beta)= \frac{\lambda_0(t)e^{z_{i0}(t)\beta}}{ \lambda_0(t)e^{z_{i0}(t)\beta}+ \lambda_0(t)e^{z_{i1}(t)\beta}}=\frac{e^{z_{i0}(t)\beta}}{e^{z_{i0}(t)\beta}+e^{z_{i1}(t)\beta}}$$

More generally, to do comparisons within a **stratum**, the denominator is only a sum over people in that stratum under observation at the same time.


(Economists call this a *fixed-effects* analysis)

---

Computation:

- `+strata(id)` in R
- `, strata(id)` in Stata
- `STRATA id;` statement in SAS PHREG

Can also be used to handle non-proportional hazards for a discrete variable: comparisons only within the same level of that variable.

## Clustered Cox model

If the clustering is a nuisance, not the primary question, we just want a Cox model for individual observations as usual

Use

- `+cluster(id)` in model formula for R
- `, cluster(id)` option for Stata
- `ID` statement and `COVS(AGGREGATE)` option in SAS

to get correct standard errors.

## More complicated

We might care about estimating the **correlation** within a cluster

- are monozygotic (identical) twins more similar than dizygotic (fraternal) twins?
- are you more likely to graduate late if your flatmates do?

This is harder, and the models depend on context.  The R `coxme` package handles heritability estimation for family-based genetics. 

## Exercise

<img src="exercise.png" height=400px>


## Questions?

<img src="weka_hires.jpg" height=500">


## Competing risks

Example: tumours are removed surgically, and the surgeon is interested whether the removal is complete enough

- surgery couldn't do anything about metastases
- surgeon wants the time to *local* recurrence, censored at death or distant recurrence

Example: blood pressure treatment reduces risk of stroke and heart disease

- researchers want time to stroke or heart attack
- deaths from other causes shouldn't be affected by treatment, use as censoring time

## Problem

Ignorable censoring assumption would say that someone who died of another cause was still at the same risk of stroke or heart attack or local recurrence as someone still alive

They aren't. They're dead. 

Ignorable censoring **can't be** true in the usual sense: competing risks aren't really censoring.

**Conceivable** that someone who died of another cause would still be at the same risk of stroke or heart attack or local recurrence as someone still alive, if they hadn't died.  Unlikely.

## Ignorable?

Why might...

- someone who had a distance cancer recurrence have been at a higher or lower risk for local recurrence?
- someone who had a heart attack have been at a higher or lower risk for cancer?

...talk to the person next to your for a couple of minutes.

## What can we estimate?

**Crude incidence**: what proportion of people get/don't get local recurrence or stroke by time $t$? Crude incidences add up to total incidence: $1-\hat{S}(t)$

**Cause-specific hazard**: rate of events from a specific cause. Cause-specific hazards add up to total hazard : $\hat{\Lambda}(t)$

Both involve removing the effect of actual censoring, but not of competing risks. 

## Not the same

Example: 

- group 1: rates of  30%/year for cancer and non-cancer deaths
- group 2: cancer rate of 20%/year, non-cancer rate of 10%/year


Cause-specific hazard of cancer is higher in group 1, but more people eventually die of cancer in group 2 because they don't die of other things first. 

Crude incidence and cause-specific hazard need not agree unless all event probabilities are low.

---

```{r echo=FALSE}
par(mfrow=c(2,1),mar=c(4,4,.1,1))
curve(3+0*x,from=0,to=3,ylim=c(0,4),xlab="Decades",ylab="Rate")
curve(1+0*x,col="red",add=TRUE)
curve((1-exp(-6*x))/2,from=0,to=3,xlab="Decades",ylab="Cumulative Incidence",ylim=c(0,1))
curve((1-exp(-3*x))*2/3,col="red",add=TRUE)

```

## Computation

- Cause-specific hazards are what you estimate if you do a Cox model 'censoring' at deaths from other causes
- Crude incidence is what you estimate if you say someone dying of one cause never dies of other causes and is censored at the end of the study -- but that gets the standard errors wrong.

R package `cmprsk` does crude-incidence comparisons and regression models

`-stcrreg-` and `-stcurve-` in Stata'

`eventcode` option to `PHREG` in SAS

## Questions?

<img src="weka_hires.jpg" height=500">


## Ties,  intervals, error

We've assumed that

- we know the start time for each person
- we know the censoring time for each person (if they don't die)
- we know the event time for each person (if they are under observation)

These aren't always true.  

We've also implicitly assumed all the event times (in a stratum) are different when writing down the partial likelihood.

## Ties from rounding

Times may get rounded off to the nearest day, week, month.

Software uses various approximations to the partial likelihood. They are all Good Enough except in extreme cases.

R and Stata default to the same approximation; SAS defaults to a cruder one. 

Worth knowing about as a potential explanation if you get slightly different results from the same data with different programs.

## Events only at visit

Some events can only be measured retrospectively by a diagnostic test

- incident diabetes
- HIV infection

We don't know the time ordering of the real events between diagnostic tests, so we don't know who goes in the denominator for the partial likelihood at each event.

If everyone has similar test schedule or test schedule is frequent enough, just pretend the events happen at the test times. 

Otherwise: interval censoring is hard.

## Retrospective assessment

Example: heart attack adjudicated by expert examination of medical records

- date and time is known immediately
- whether it is an event isn't known until later.

Example: vital status from  death certificate records

- nothing known immediately
- date and time will be known, but not for a while

The censoring time should be the last time you **know** the event hadn't happened

## Measurement error

- event may be false positive
- event may be missed
- start time may be wrong (recall error, eg start time for antiretroviral treatment in HIV)

These are hard if the errors are large/frequent.  Ask an expert. 


## Exercise

<img src="exercise.png" height=400px>


## Questions?

<img src="weka_hires.jpg" height=500">


## Parametric survival models


## Exercise

<img src="exercise.png" height=400px>


## Questions?

<img src="weka_hires.jpg" height=500">

## Subsampling


